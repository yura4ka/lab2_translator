{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "I45xQuvEAYEd",
    "outputId": "64dbab74-473f-47fc-d0be-d1201519202a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch==2.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: torchvision==0.18.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (0.18.0+cu121)\n",
      "Requirement already satisfied: torchaudio==2.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch==2.3.0) (2021.4.0)\n",
      "Requirement already satisfied: numpy in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torchvision==0.18.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torchvision==0.18.0) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from sympy->torch==2.3.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1OcC4k82CMBE",
    "outputId": "954a1f5e-aa6b-4089-b567-e8ae78ba49f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.18.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torchtext==0.18.0) (4.67.0)\n",
      "Requirement already satisfied: requests in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torchtext==0.18.0) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torchtext==0.18.0) (2.3.0+cu121)\n",
      "Requirement already satisfied: numpy in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torchtext==0.18.0) (2.0.2)\n",
      "Requirement already satisfied: filelock in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from torch>=2.3.0->torchtext==0.18.0) (2021.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests->torchtext==0.18.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests->torchtext==0.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests->torchtext==0.18.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests->torchtext==0.18.0) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from tqdm->torchtext==0.18.0) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.3.0->torchtext==0.18.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.3.0->torchtext==0.18.0) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from jinja2->torch>=2.3.0->torchtext==0.18.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from sympy->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "fpZRcsunA2js",
    "outputId": "777a67d5-b212-4fae-b6fa-86b2d2ffb9ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C4qSXjlNBWTj",
    "outputId": "52a9ad32-403b-44b6-cad8-f2a991e52dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\uni\\lyng\\lab2\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download uk_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EERRpTusBmNV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rRBsHIkuBpws",
    "outputId": "210e4667-3747-497d-fc1c-7aa07491781c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>uk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let's try something.</td>\n",
       "      <td>Давайте щось спробуємо!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>Маю піти спати.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>Мюріел зараз двадцять.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The password is \"Muiriel\".</td>\n",
       "      <td>Пароль - \"Muiriel\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I will be back soon.</td>\n",
       "      <td>Я скоро повернуся.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           en                       uk\n",
       "0        Let's try something.  Давайте щось спробуємо!\n",
       "1      I have to go to sleep.          Маю піти спати.\n",
       "2          Muiriel is 20 now.   Мюріел зараз двадцять.\n",
       "3  The password is \"Muiriel\".      Пароль - \"Muiriel\".\n",
       "4        I will be back soon.       Я скоро повернуся."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC = \"en\"\n",
    "TGT = \"uk\"\n",
    "csv = pd.read_csv(\"../data/en-uk.tsv\", sep=\"\\t\", usecols=[1, 3], names=[SRC, TGT])\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "35AM3L9SCH58"
   },
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "uk_tokenizer = get_tokenizer(\"spacy\", language=\"uk_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONYvF0-DDEPU",
    "outputId": "987798e7-f679-43a8-d4df-079f546a3833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18523\n",
      "49759\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(data, tokenizer):\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        counter.update(tokenizer(d))\n",
    "    print(len(counter))\n",
    "    return vocab(\n",
    "        counter, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], special_first=True\n",
    "    )\n",
    "\n",
    "\n",
    "en_vocab = build_vocab(csv[SRC], en_tokenizer)\n",
    "uk_vocab = build_vocab(csv[TGT], uk_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GK3U6E6XItWL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\yurii\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\yurii\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\yurii\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\yurii\\AppData\\Local\\Temp\\ipykernel_17596\\844143816.py\", line 16, in <module>\n",
      "    data = data_process(csv)\n",
      "  File \"C:\\Users\\yurii\\AppData\\Local\\Temp\\ipykernel_17596\\844143816.py\", line 6, in data_process\n",
      "    en_tensor_ = torch.tensor(\n",
      "C:\\Users\\yurii\\AppData\\Local\\Temp\\ipykernel_17596\\844143816.py:6: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  en_tensor_ = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "def data_process(csv):\n",
    "    raw_en_iter = iter(csv[SRC])\n",
    "    raw_uk_iter = iter(csv[TGT])\n",
    "    data = []\n",
    "    for raw_en, raw_uk in zip(raw_en_iter, raw_uk_iter):\n",
    "        en_tensor_ = torch.tensor(\n",
    "            [en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long\n",
    "        )\n",
    "        uk_tensor_ = torch.tensor(\n",
    "            [uk_vocab[token] for token in uk_tokenizer(raw_uk)], dtype=torch.long\n",
    "        )\n",
    "        data.append((en_tensor_, uk_tensor_))\n",
    "    return data\n",
    "\n",
    "\n",
    "data = data_process(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-xvppMILrWD",
    "outputId": "44f6d9fb-e751-4d85-f6b2-fd8345b9d8d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "PAD_IDX = en_vocab[\"<pad>\"]\n",
    "BOS_IDX = en_vocab[\"<bos>\"]\n",
    "EOS_IDX = en_vocab[\"<eos>\"]\n",
    "print(PAD_IDX, uk_vocab[\"<pad>\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AYEiqJwsLxfj"
   },
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    en_batch, uk_batch = [], []\n",
    "    for en_item, uk_item in data_batch:\n",
    "        en_batch.append(\n",
    "            torch.cat(\n",
    "                [torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0\n",
    "            )\n",
    "        )\n",
    "        uk_batch.append(\n",
    "            torch.cat(\n",
    "                [torch.tensor([BOS_IDX]), uk_item, torch.tensor([EOS_IDX])], dim=0\n",
    "            )\n",
    "        )\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    uk_batch = pad_sequence(uk_batch, padding_value=PAD_IDX)\n",
    "    return en_batch, uk_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iUG1z1mFMk6y"
   },
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    data, [0.8, 0.1, 0.1]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-M3FnovNRyT",
    "outputId": "56cf45bf-b296-4d86-8c54-78759c9ffe9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683 171712\n",
      "Shape of en: torch.Size([20, 64]) torch.int64\n",
      "Shape of uk: torch.Size([15, 64]) torch.int64\n",
      "['<bos>', 'He', 'will', 'be', 'a', 'good', 'teacher', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'Він', 'стане', 'хорошим', 'викладачем', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(train_loader) * BATCH_SIZE)\n",
    "for en, uk in test_loader:\n",
    "    en_itos = en_vocab.get_itos()\n",
    "    uk_itos = uk_vocab.get_itos()\n",
    "    print(f\"Shape of en: {en.shape} {en.dtype}\")\n",
    "    print(f\"Shape of uk: {uk.shape} {uk.dtype}\")\n",
    "    print([en_itos[w] for w in en.T[0]])\n",
    "    print([uk_itos[w] for w in uk.T[0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mzsi0GIWVBKa"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nOmgc6GzlfwI"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        emb_size,\n",
    "        nhead,\n",
    "        ff_dim,\n",
    "        num_enc_layers,\n",
    "        num_dec_layers,\n",
    "        pad_idx,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.src_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_enc_layers,\n",
    "            num_decoder_layers=num_dec_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        src = self.positional_encoding(self.src_emb(src))\n",
    "        tgt = self.positional_encoding(self.tgt_emb(tgt))\n",
    "\n",
    "        output = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "        )\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def create_masks(self, src, tgt):\n",
    "        src_seq_len, tgt_seq_len = src.size(0), tgt.size(0)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=src.device).type(\n",
    "            torch.bool\n",
    "        )\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "\n",
    "        src_padding_mask = (src == self.pad_idx).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt == self.pad_idx).transpose(0, 1)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        return torch.triu(torch.ones(size, size) * float(\"-inf\"), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQYp8X6rmtho",
    "outputId": "75a1d074-f215-4bef-cd40-5dd0603f6187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 18527 49763\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SRC_VOCAB_SIZE = len(en_vocab)\n",
    "TGT_VOCAB_SIZE = len(uk_vocab)\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "print(DEVICE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVbT4TODmA5D",
    "outputId": "27d6fcfa-1e6d-46b1-a34e-2644673e6f44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    emb_size=EMB_SIZE,\n",
    "    nhead=NHEAD,\n",
    "    ff_dim=FFN_HID_DIM,\n",
    "    num_enc_layers=NUM_ENCODER_LAYERS,\n",
    "    num_dec_layers=NUM_DECODER_LAYERS,\n",
    "    pad_idx=PAD_IDX,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(step):\n",
    "    warmup_steps = 4000\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YC2mxi8rEh5",
    "outputId": "d2db8188-734f-403b-c561-1bc21a7a51d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 24,501,155 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "83eTC7ydoCd5"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in tqdm(train_loader, total=len(train_loader)):\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_output = tgt[1:, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = model.create_masks(\n",
    "            src, tgt_input\n",
    "        )\n",
    "        logits = model(\n",
    "            src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)), tgt_output.contiguous().view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, valid_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(valid_loader, total=len(valid_loader)):\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            tgt_output = tgt[1:, :]\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = model.create_masks(\n",
    "                src, tgt_input\n",
    "            )\n",
    "            logits = model(\n",
    "                src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "            )\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)), tgt_output.contiguous().view(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6M-6ewa2s40w"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuOeey5-qLVQ",
    "outputId": "3ef7372e-6699-4708-d802-82b5388293a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2683 [00:00<?, ?it/s]d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "d:\\uni\\lyng\\lab2\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2683/2683 [03:03<00:00, 14.65it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 3m 11s\n",
      "\tTrain Loss: 4.832 | Train PPL: 125.422\n",
      "\t Val. Loss: 4.042 |  Val. PPL:  56.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:16<00:00,  8.48it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 46.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 5m 23s\n",
      "\tTrain Loss: 3.895 | Train PPL:  49.143\n",
      "\t Val. Loss: 3.591 |  Val. PPL:  36.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:28<00:00,  8.16it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 47.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 5m 36s\n",
      "\tTrain Loss: 3.530 | Train PPL:  34.127\n",
      "\t Val. Loss: 3.291 |  Val. PPL:  26.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:25<00:00,  8.24it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 47.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 5m 32s\n",
      "\tTrain Loss: 3.278 | Train PPL:  26.518\n",
      "\t Val. Loss: 3.074 |  Val. PPL:  21.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:33<00:00,  8.05it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 46.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 5m 40s\n",
      "\tTrain Loss: 3.086 | Train PPL:  21.899\n",
      "\t Val. Loss: 2.905 |  Val. PPL:  18.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:24<00:00,  8.26it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 46.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 5m 32s\n",
      "\tTrain Loss: 2.932 | Train PPL:  18.757\n",
      "\t Val. Loss: 2.775 |  Val. PPL:  16.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:29<00:00,  8.15it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 46.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 5m 36s\n",
      "\tTrain Loss: 2.804 | Train PPL:  16.507\n",
      "\t Val. Loss: 2.655 |  Val. PPL:  14.220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:32<00:00,  8.06it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 5m 39s\n",
      "\tTrain Loss: 2.698 | Train PPL:  14.850\n",
      "\t Val. Loss: 2.578 |  Val. PPL:  13.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:31<00:00,  8.09it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 47.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 5m 38s\n",
      "\tTrain Loss: 2.610 | Train PPL:  13.595\n",
      "\t Val. Loss: 2.505 |  Val. PPL:  12.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:25<00:00,  8.23it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 47.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 5m 33s\n",
      "\tTrain Loss: 2.535 | Train PPL:  12.621\n",
      "\t Val. Loss: 2.444 |  Val. PPL:  11.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:21<00:00,  8.33it/s]\n",
      "100%|██████████| 336/336 [00:07<00:00, 46.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 5m 29s\n",
      "\tTrain Loss: 2.470 | Train PPL:  11.823\n",
      "\t Val. Loss: 2.395 |  Val. PPL:  10.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [05:24<00:00,  8.26it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 5m 31s\n",
      "\tTrain Loss: 2.413 | Train PPL:  11.163\n",
      "\t Val. Loss: 2.352 |  Val. PPL:  10.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:46<00:00,  9.37it/s]\n",
      "100%|██████████| 336/336 [00:05<00:00, 56.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 4m 52s\n",
      "\tTrain Loss: 2.363 | Train PPL:  10.620\n",
      "\t Val. Loss: 2.313 |  Val. PPL:  10.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:46<00:00,  9.37it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 4m 52s\n",
      "\tTrain Loss: 2.319 | Train PPL:  10.164\n",
      "\t Val. Loss: 2.277 |  Val. PPL:   9.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:44<00:00,  9.43it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 4m 50s\n",
      "\tTrain Loss: 2.279 | Train PPL:   9.771\n",
      "\t Val. Loss: 2.251 |  Val. PPL:   9.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:39<00:00,  9.61it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time: 4m 45s\n",
      "\tTrain Loss: 2.243 | Train PPL:   9.420\n",
      "\t Val. Loss: 2.218 |  Val. PPL:   9.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:41<00:00,  9.52it/s]\n",
      "100%|██████████| 336/336 [00:05<00:00, 56.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Time: 4m 47s\n",
      "\tTrain Loss: 2.211 | Train PPL:   9.129\n",
      "\t Val. Loss: 2.195 |  Val. PPL:   8.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:49<00:00,  9.27it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Time: 4m 55s\n",
      "\tTrain Loss: 2.185 | Train PPL:   8.890\n",
      "\t Val. Loss: 2.179 |  Val. PPL:   8.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:42<00:00,  9.48it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Time: 4m 48s\n",
      "\tTrain Loss: 2.160 | Train PPL:   8.671\n",
      "\t Val. Loss: 2.165 |  Val. PPL:   8.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [04:45<00:00,  9.40it/s]\n",
      "100%|██████████| 336/336 [00:06<00:00, 55.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Time: 4m 51s\n",
      "\tTrain Loss: 2.137 | Train PPL:   8.476\n",
      "\t Val. Loss: 2.156 |  Val. PPL:   8.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_loader)\n",
    "    valid_loss = evaluate(model, criterion, valid_loader)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../outputs/model_new1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_full(epoch):\n",
    "  torch.save({\n",
    "    'epoch': epoch,\n",
    "    'mode_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "  }, f\"../outputs/model_full_new_epoch{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_full(NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:11<00:00,  6.22it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Time: 7m 19s\n",
      "\tTrain Loss: 2.081 | Train PPL:   8.009\n",
      "\t Val. Loss: 2.126 |  Val. PPL:   8.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:14<00:00,  6.18it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Time: 7m 22s\n",
      "\tTrain Loss: 2.081 | Train PPL:   8.016\n",
      "\t Val. Loss: 2.171 |  Val. PPL:   8.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:13<00:00,  6.19it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Time: 7m 22s\n",
      "\tTrain Loss: 2.108 | Train PPL:   8.232\n",
      "\t Val. Loss: 2.187 |  Val. PPL:   8.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:07<00:00,  6.28it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Time: 7m 15s\n",
      "\tTrain Loss: 2.117 | Train PPL:   8.303\n",
      "\t Val. Loss: 2.181 |  Val. PPL:   8.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:09<00:00,  6.25it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Time: 7m 17s\n",
      "\tTrain Loss: 2.118 | Train PPL:   8.314\n",
      "\t Val. Loss: 2.192 |  Val. PPL:   8.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:04<00:00,  6.32it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Time: 7m 12s\n",
      "\tTrain Loss: 2.113 | Train PPL:   8.275\n",
      "\t Val. Loss: 2.187 |  Val. PPL:   8.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:10<00:00,  6.24it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Time: 7m 18s\n",
      "\tTrain Loss: 2.109 | Train PPL:   8.240\n",
      "\t Val. Loss: 2.188 |  Val. PPL:   8.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:18<00:00,  6.12it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Time: 7m 26s\n",
      "\tTrain Loss: 2.104 | Train PPL:   8.203\n",
      "\t Val. Loss: 2.186 |  Val. PPL:   8.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:08<00:00,  6.26it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Time: 7m 16s\n",
      "\tTrain Loss: 2.097 | Train PPL:   8.140\n",
      "\t Val. Loss: 2.186 |  Val. PPL:   8.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:06<00:00,  6.29it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Time: 7m 15s\n",
      "\tTrain Loss: 2.092 | Train PPL:   8.099\n",
      "\t Val. Loss: 2.185 |  Val. PPL:   8.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:03<00:00,  6.33it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Time: 7m 11s\n",
      "\tTrain Loss: 2.087 | Train PPL:   8.064\n",
      "\t Val. Loss: 2.182 |  Val. PPL:   8.861\n",
      "saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:05<00:00,  6.31it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Time: 7m 13s\n",
      "\tTrain Loss: 2.081 | Train PPL:   8.011\n",
      "\t Val. Loss: 2.186 |  Val. PPL:   8.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:14<00:00,  6.18it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Time: 7m 22s\n",
      "\tTrain Loss: 2.076 | Train PPL:   7.969\n",
      "\t Val. Loss: 2.189 |  Val. PPL:   8.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:10<00:00,  6.23it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Time: 7m 19s\n",
      "\tTrain Loss: 2.072 | Train PPL:   7.942\n",
      "\t Val. Loss: 2.188 |  Val. PPL:   8.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:07<00:00,  6.27it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Time: 7m 15s\n",
      "\tTrain Loss: 2.068 | Train PPL:   7.912\n",
      "\t Val. Loss: 2.190 |  Val. PPL:   8.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:07<00:00,  6.28it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Time: 7m 15s\n",
      "\tTrain Loss: 2.065 | Train PPL:   7.883\n",
      "\t Val. Loss: 2.189 |  Val. PPL:   8.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:11<00:00,  6.22it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Time: 7m 19s\n",
      "\tTrain Loss: 2.062 | Train PPL:   7.862\n",
      "\t Val. Loss: 2.194 |  Val. PPL:   8.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:13<00:00,  6.19it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Time: 7m 21s\n",
      "\tTrain Loss: 2.058 | Train PPL:   7.830\n",
      "\t Val. Loss: 2.189 |  Val. PPL:   8.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:14<00:00,  6.17it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Time: 7m 23s\n",
      "\tTrain Loss: 2.056 | Train PPL:   7.816\n",
      "\t Val. Loss: 2.193 |  Val. PPL:   8.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:13<00:00,  6.19it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Time: 7m 21s\n",
      "\tTrain Loss: 2.052 | Train PPL:   7.783\n",
      "\t Val. Loss: 2.190 |  Val. PPL:   8.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:10<00:00,  6.23it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Time: 7m 19s\n",
      "\tTrain Loss: 2.050 | Train PPL:   7.771\n",
      "\t Val. Loss: 2.192 |  Val. PPL:   8.949\n",
      "saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:08<00:00,  6.26it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Time: 7m 16s\n",
      "\tTrain Loss: 2.047 | Train PPL:   7.744\n",
      "\t Val. Loss: 2.192 |  Val. PPL:   8.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:15<00:00,  6.16it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Time: 7m 24s\n",
      "\tTrain Loss: 2.045 | Train PPL:   7.733\n",
      "\t Val. Loss: 2.193 |  Val. PPL:   8.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:05<00:00,  6.31it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Time: 7m 13s\n",
      "\tTrain Loss: 2.043 | Train PPL:   7.716\n",
      "\t Val. Loss: 2.196 |  Val. PPL:   8.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:04<00:00,  6.31it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Time: 7m 13s\n",
      "\tTrain Loss: 2.041 | Train PPL:   7.696\n",
      "\t Val. Loss: 2.201 |  Val. PPL:   9.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:07<00:00,  6.28it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Time: 7m 15s\n",
      "\tTrain Loss: 2.038 | Train PPL:   7.675\n",
      "\t Val. Loss: 2.202 |  Val. PPL:   9.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:08<00:00,  6.26it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 40.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Time: 7m 16s\n",
      "\tTrain Loss: 2.037 | Train PPL:   7.667\n",
      "\t Val. Loss: 2.200 |  Val. PPL:   9.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:09<00:00,  6.24it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 39.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Time: 7m 18s\n",
      "\tTrain Loss: 2.035 | Train PPL:   7.654\n",
      "\t Val. Loss: 2.196 |  Val. PPL:   8.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:13<00:00,  6.19it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Time: 7m 21s\n",
      "\tTrain Loss: 2.034 | Train PPL:   7.645\n",
      "\t Val. Loss: 2.196 |  Val. PPL:   8.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2683/2683 [07:09<00:00,  6.24it/s]\n",
      "100%|██████████| 336/336 [00:08<00:00, 41.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Time: 7m 17s\n",
      "\tTrain Loss: 2.031 | Train PPL:   7.621\n",
      "\t Val. Loss: 2.201 |  Val. PPL:   9.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS, 50):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_loader)\n",
    "    valid_loss = evaluate(model, criterion, valid_loader)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")\n",
    "\n",
    "    if epoch % 10 == 0 and epoch != NUM_EPOCHS:\n",
    "        print(\"saving...\")\n",
    "        save_model_full(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_full(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../outputs/model_full_new_epoch50.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint['mode_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "uk_vocab.set_default_index(uk_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input: str, len_diff = 10):\n",
    "  model.eval()\n",
    "\n",
    "  indices = torch.tensor(\n",
    "    [en_vocab[token] for token in en_tokenizer(input)], dtype=torch.long\n",
    "  )\n",
    "\n",
    "  src = torch.cat(\n",
    "    [torch.tensor([BOS_IDX]), indices, torch.tensor([EOS_IDX])], dim=0\n",
    "  ).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "  num_tokens = src.shape[0]\n",
    "  src_mask = (torch.zeros(num_tokens, num_tokens, device=DEVICE)).type(torch.bool)\n",
    "  src_padding_mask = (src == en_vocab[\"<pad>\"]).transpose(0, 1).type(torch.bool).to(DEVICE)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    memory = model.transformer.encoder(\n",
    "      model.positional_encoding(model.src_emb(src)),\n",
    "      mask=src_mask,\n",
    "      src_key_padding_mask=src_padding_mask,\n",
    "    )\n",
    "\n",
    "  tgt_indices = (\n",
    "    torch.tensor([uk_vocab[\"<bos>\"]], dtype=torch.long).unsqueeze(1).to(DEVICE)\n",
    "  ).to(DEVICE)\n",
    "  memory = memory.to(DEVICE)\n",
    "\n",
    "  for _ in range(num_tokens + len_diff):\n",
    "    tgt_mask = model.generate_square_subsequent_mask(tgt_indices.size(0)).type(torch.bool).to(DEVICE)\n",
    "    tgt_padding_mask = (tgt_indices == uk_vocab[\"<pad>\"]).transpose(0, 1).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = model.transformer.decoder(\n",
    "        model.positional_encoding(model.tgt_emb(tgt_indices)),\n",
    "        memory,\n",
    "        tgt_mask=tgt_mask,\n",
    "        tgt_key_padding_mask=tgt_padding_mask\n",
    "      )\n",
    "      logits = model.fc_out(output[-1])\n",
    "      next_token = logits.argmax(-1).item()\n",
    "\n",
    "    tgt_indices = torch.cat(\n",
    "      [tgt_indices, torch.tensor([[next_token]], device=DEVICE)], dim=0\n",
    "    )\n",
    "\n",
    "    if next_token == uk_vocab[\"<eos>\"]:\n",
    "      break\n",
    "\n",
    "  translated_sentence = [\n",
    "    uk_vocab.lookup_token(idx)\n",
    "    for idx in tgt_indices.squeeze().tolist()\n",
    "    if idx not in {uk_vocab[\"<bos>\"], uk_vocab[\"<eos>\"]}\n",
    "  ]\n",
    "\n",
    "  return \" \".join(translated_sentence)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very cool ======== Я дуже спокій .\n",
      "Is it working? ======== Це працює ?\n",
      "I am a good teacher ======== Я добрий учитель .\n",
      "He likes apples ======== Він любить яблука .\n",
      "Those who stand with me shall be my brothers ======== Ці брати мене були в тому , хто з моїм братів .\n",
      "Despair for your end is near ======== З твого боку знаходиться поруч із собою .\n",
      "What is the weather today? ======== Яка сьогодні погода ?\n",
      "She is good at it ======== Вона добре грає на неї .\n",
      "Today was great ======== Сьогодні було чудово .\n",
      "Call me! ======== Зателефонуй мені !\n",
      "People stopped telling jokes ======== Люди зупинив жарти , щоб сказати .\n",
      "Well, I am not very happy with the results ======== Ну що ж не дуже щасливий , я щасливий з собою .\n",
      "But this is probably okay ======== Але це , мабуть , в порядку .\n",
      "I will try to do another one ======== Я спробую ще один зробити .\n",
      "It will take a whole night ======== Це буде ніч .\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "  \"I am very cool\",\n",
    "  \"Is it working?\",\n",
    "  \"I am a good teacher\",\n",
    "  \"He likes apples\",\n",
    "  \"Those who stand with me shall be my brothers\",\n",
    "  \"Despair for your end is near\",\n",
    "  \"What is the weather today?\",\n",
    "  \"She is good at it\",\n",
    "  \"Today was great\",\n",
    "  \"Call me!\",\n",
    "  \"People stopped telling jokes\",\n",
    "  \"Well, I am not very happy with the results\",\n",
    "  \"But this is probably okay\",\n",
    "  \"I will try to do another one\",\n",
    "  \"It will take a whole night\"\n",
    "]\n",
    "\n",
    "\n",
    "for s in samples:\n",
    "  print(s, \"========\", translate(s))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
